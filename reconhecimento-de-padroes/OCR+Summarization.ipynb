{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8184733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jambi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jambi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR...\n",
      "OCR Result:\n",
      "2.1\n",
      "lnlrodugio\n",
      "\n",
      "Na maiuria das aplicaqées leans as slsssmsasas, prevnsim, apmximacﬁo s\n",
      "smmusas, as basss as dados oonlém um gmnde mimem as camlerisncas, munas\n",
      "delas inlmduzidas pam obler uma mellmr represemacﬁo do pmblema, ms Como,\n",
      "por exemplo, name, idenlidade, endereco, etc, Enlretanlo, na rnamria ass casns,\n",
      "gmnde pane deslas camlerislncas sas inslmmss elou mdundanles Desle modo,\n",
      "\n",
      "urn pmblerna comum nestas aplicaqées reais s a seleeﬁo das camclerislicas,\n",
      "\n",
      "A selecﬁo as camclerislicas ss rsrsrs a urn pmcesso no qual um sspass as\n",
      "dados s Lransfonnado em um espago as camclerislicas, as menor aimsnsas, mas\n",
      "qus ainda relenha a rnaior pane da infcn-nacﬁo imrinseca ass dados; sm outms\n",
      "palavras, o conjunlo as dados sofre urna reducio as djmensmnalidade, Os\n",
      "mélodos as sslssaa as caraclerisncas Iramm exalamenle da escolha, demre lodos\n",
      "ss alribulos da base as dados, daqueles mans relevanles do ponlo as vista da\n",
      "\n",
      "mrsnnas.-as [MARD79], [DASH97]\n",
      "\n",
      "\n",
      "\n",
      "Performing Summarization...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jambi/nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jambi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Realizar sumarização\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerforming Summarization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m summary_text = \u001b[43mperform_summarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(summary_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mperform_summarization\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mperform_summarization\u001b[39m(text):\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Tokenize o texto em frases, divide o texto em frases usando a função sent_tokenize da biblioteca NLTK, considerando o idioma ingles padrao.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Tokenize as palavras, divide o texto em palavras usando a função word_tokenize da biblioteca NLTK.\u001b[39;00m\n\u001b[32m     29\u001b[39m     words = word_tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jambi\\OneDrive\\Documentos\\repositorios-git\\sptech\\4CCO\\reconhecimento-de-padroes\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jambi\\OneDrive\\Documentos\\repositorios-git\\sptech\\4CCO\\reconhecimento-de-padroes\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jambi\\OneDrive\\Documentos\\repositorios-git\\sptech\\4CCO\\reconhecimento-de-padroes\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jambi\\OneDrive\\Documentos\\repositorios-git\\sptech\\4CCO\\reconhecimento-de-padroes\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jambi\\OneDrive\\Documentos\\repositorios-git\\sptech\\4CCO\\reconhecimento-de-padroes\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jambi/nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\jambi\\\\OneDrive\\\\Documentos\\\\repositorios-git\\\\sptech\\\\4CCO\\\\reconhecimento-de-padroes\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jambi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "\n",
    "# Carregar modelo de linguagem do SpaCy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Baixar as stopwords do NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Função para realizar OCR em uma imagem\n",
    "def perform_ocr(image_path):\n",
    "    # Abrir a imagem\n",
    "    image = Image.open(image_path)\n",
    "    # Realizar OCR usando Tesseract\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "# Função para realizar a sumarização de texto\n",
    "def perform_summarization(text):\n",
    "    # Tokenize o texto em frases, divide o texto em frases usando a função sent_tokenize da biblioteca NLTK, considerando o idioma ingles padrao.\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Tokenize as palavras, divide o texto em palavras usando a função word_tokenize da biblioteca NLTK.\n",
    "    words = word_tokenize(text)\n",
    "    # Remova stopwords, remove as palavras irrelevantes (stop words) do texto.\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Calcule a frequência das palavras, calcula a frequência das palavras no texto usando a classe FreqDist da biblioteca NLTK.\n",
    "    freq_dist = FreqDist(words)\n",
    "    # Calcule a frequência máxima, calcula a frequência máxima entre todas as palavras no texto.\n",
    "    max_freq = max(freq_dist.values())\n",
    "    # Normalizar as frequências, normaliza as frequências das palavras dividindo a frequência de cada palavra pela frequência máxima.\n",
    "    for word in freq_dist.keys():\n",
    "        freq_dist[word] = (freq_dist[word]/max_freq)\n",
    "    # Calcule a pontuação das frases\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        #Divide a frase em palavras, converte para minúsculas\n",
    "        for word in word_tokenize(sentence.lower(), language='english'):  \n",
    "            #Verifica se a palavra está presente nas palavras relevantes (que não são stop words).\n",
    "            if word in freq_dist.keys():\n",
    "                #Verifica se a frase tem menos de 30 palavras.\n",
    "                if len(sentence.split(' ')) < 30: \n",
    "                    #Verifica se a frase não está presente no dicionário de pontuações das frases.\n",
    "                    if sentence not in sentence_scores.keys(): \n",
    "                        #Atribui a pontuação da palavra à frase no dicionário de pontuações das frases.\n",
    "                        sentence_scores[sentence] = freq_dist[word] \n",
    "                    #Se a frase já estiver presente no dicionário de pontuações das frases.\n",
    "                    else:\n",
    "                        #Incrementa a pontuação da frase com a pontuação da palavra.\n",
    "                        sentence_scores[sentence] += freq_dist[word] \n",
    "    # Obtenha as frases mais importantes\n",
    "    #Ordena as frases com base em suas pontuações, do maior para o menor, e seleciona as três primeiras frases.\n",
    "    summarized_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
    "    #Une as frases selecionadas em um único resumo, separadas por espaço.\n",
    "    summary = ' '.join(summarized_sentences) \n",
    "    return summary\n",
    "\n",
    "# Caminho da imagem\n",
    "image_path = \"texto.png\"\n",
    "\n",
    "# Realizar OCR\n",
    "print(\"Performing OCR...\")\n",
    "ocr_text = perform_ocr(image_path)\n",
    "print(\"OCR Result:\")\n",
    "print(ocr_text)\n",
    "print()\n",
    "\n",
    "# Realizar sumarização\n",
    "print(\"Performing Summarization...\")\n",
    "summary_text = perform_summarization(ocr_text)\n",
    "print(\"Summary:\")\n",
    "print(summary_text)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc30eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e2b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
